{"cells":[{"cell_type":"markdown","id":"9f40c232-df6e-49df-9016-6459e4af2e1e","metadata":{"tags":[],"id":"9f40c232-df6e-49df-9016-6459e4af2e1e"},"source":["# Coding Self-Attention in PyTroch!!!\n","\n"]},{"cell_type":"markdown","id":"cc125fb6-069e-4fc1-b153-af417c535b2c","metadata":{"id":"cc125fb6-069e-4fc1-b153-af417c535b2c"},"source":["<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"]},{"cell_type":"markdown","id":"f4226d63-8d76-40bc-a8e6-0f290a159418","metadata":{"id":"f4226d63-8d76-40bc-a8e6-0f290a159418"},"source":["----\n","\n","In this tutorial, we will code **Self-Attention** in **[PyTorch](https://pytorch.org/)**. **Attention** is an essential component of neural network **Transformers**, which are driving the current excitement in **Large Language Models** and **AI**. Specifically, an **Enecoder-Only Transformer**, illustrated below, is the foundation for the popular model **BERT**.\n","\n","<img src=\"./images/encoder_only_1.png\" alt=\"an enecoder-only transformer neural network\" style=\"width: 800px;\">\n","\n","At the heart of **BERT** is **Self-Attention**, which allows it to establish relationships among the words, characters and symbols, that are used for input and collectively called **Tokens**. For example, in the illustration below, where the word **it** could potentially refer to either **pizza** or **oven**, **Attention** could help a **Transformer** establish the correctly relationship between the word **it** and **pizza**.\n","\n","<img src=\"./images/attention_ex_1.png\" alt=\"an illustration of how attention works\" style=\"width: 800px;\"/>\n","\n","In this tutorial, you will...\n","\n","- **[Code a Basic Self-Attention Class!!!](#selfAttention)** The basic self-attention class allows the transformer to establish relationships among words and tokens.\n","\n","- **[Calculate Self-Attention Values!!!](#calculate)** We'll then use the class that we created, SelfAttention, to calculate self-attention values for some sample data.\n","\n","- **[Verify The Calculations!!!](#validate)** Lastly, we'll validate the calculations made by the SelfAttention class..\n"]},{"cell_type":"markdown","id":"855d3a52-a43e-44cf-b8b6-b2ce88bea382","metadata":{"id":"855d3a52-a43e-44cf-b8b6-b2ce88bea382"},"source":["----"]},{"cell_type":"markdown","id":"63b86036-1369-441c-a14d-3ba7bdaa103b","metadata":{"tags":[],"id":"63b86036-1369-441c-a14d-3ba7bdaa103b"},"source":["# Import the modules that will do all the work"]},{"cell_type":"code","source":[],"metadata":{"id":"9b5wcAFjGrIL"},"id":"9b5wcAFjGrIL","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"5c520e0b-c6e4-43ce-93f5-c0f2b5e75438","metadata":{"height":64,"id":"5c520e0b-c6e4-43ce-93f5-c0f2b5e75438"},"outputs":[],"source":["import torch ## torch let's us create tensors and also provides helper functions\n","import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n","import torch.nn.functional as F # This gives us the softmax()"]},{"cell_type":"markdown","id":"56b01434-9796-4f5f-9d39-341505c912d6","metadata":{"id":"56b01434-9796-4f5f-9d39-341505c912d6"},"source":["<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> üíª &nbsp; <b>Access <code>requirements.txt</code> file:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix - Tips and Help\"</em> Lesson.</p>"]},{"cell_type":"markdown","id":"6e58ebb2-1798-41c3-9ff8-1b4f50605964","metadata":{"id":"6e58ebb2-1798-41c3-9ff8-1b4f50605964"},"source":["----"]},{"cell_type":"markdown","id":"125c4f0d-83b6-488f-8d91-66c54776bf2f","metadata":{"id":"125c4f0d-83b6-488f-8d91-66c54776bf2f"},"source":["# Code Self-Attention\n","<a id=\"selfAttention\"></a>"]},{"cell_type":"code","execution_count":null,"id":"e3392130-cd25-4000-97bb-9612764c83a8","metadata":{"height":863,"id":"e3392130-cd25-4000-97bb-9612764c83a8"},"outputs":[],"source":["class SelfAttention(nn.Module):\n","\n","    def __init__(self, d_model=2,\n","                 row_dim=0,\n","                 col_dim=1):\n","        ## d_model = the number of embedding values per token.\n","        ##           Because we want to be able to do the math by hand, we've\n","        ##           the default value for d_model=2.\n","        ##           However, in \"Attention Is All You Need\" d_model=512\n","        ##\n","        ## row_dim, col_dim = the indices we should use to access rows or columns\n","\n","\n","        super().__init__()\n","\n","        ## Initialize the Weights (W) that we'll use to create the\n","        ## query (q), key (k) and value (v) for each token\n","        ## NOTE: A lot of implementations include bias terms when\n","        ##       creating the the queries, keys, and values, but\n","        ##       the original manuscript that described Attention,\n","        ##       \"Attention Is All You Need\" did not, so we won't either\n","        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n","        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n","        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n","\n","        self.row_dim = row_dim\n","        self.col_dim = col_dim\n","\n","\n","    def forward(self, token_encodings):\n","        ## Create the query, key and values using the encoding numbers\n","        ## associated with each token (token encodings)\n","        q = self.W_q(token_encodings)\n","        k = self.W_k(token_encodings)\n","        v = self.W_v(token_encodings)\n","\n","        ## Compute similarities scores: (q * k^T)\n","        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n","\n","        ## Scale the similarities by dividing by sqrt(k.col_dim)\n","        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n","\n","        ## Apply softmax to determine what percent of each tokens' value to\n","        ## use in the final attention values.\n","        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n","\n","        ## Scale the values by their associated percentages and add them up.\n","        attention_scores = torch.matmul(attention_percents, v)\n","\n","        return attention_scores"]},{"cell_type":"markdown","id":"371cd1e0-0216-438d-a241-05533e4b374d","metadata":{"id":"371cd1e0-0216-438d-a241-05533e4b374d"},"source":["----"]},{"cell_type":"markdown","id":"c75aaaa3-29e0-4c03-8a11-816bd4b7aea3","metadata":{"id":"c75aaaa3-29e0-4c03-8a11-816bd4b7aea3"},"source":["# Calculate Self-Attention\n","<a id=\"calculate\"></a>"]},{"cell_type":"code","execution_count":null,"id":"6c17a35a-ceee-4502-9a25-9adbe6a0a1a8","metadata":{"height":268,"colab":{"base_uri":"https://localhost:8080/"},"id":"6c17a35a-ceee-4502-9a25-9adbe6a0a1a8","executionInfo":{"status":"ok","timestamp":1740168288983,"user_tz":0,"elapsed":203,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"759886ac-0451-4109-f0b6-48ee794fd81e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0100, 1.0641],\n","        [0.2040, 0.7057],\n","        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":4}],"source":["## create a matrix of token encodings...\n","encodings_matrix = torch.tensor([[1.16, 0.23],\n","                                 [0.57, 1.36],\n","                                 [4.41, -2.16]])\n","\n","## set the seed for the random number generator\n","torch.manual_seed(42)\n","\n","## create a basic self-attention ojbect\n","selfAttention = SelfAttention(d_model=2,\n","                               row_dim=0,\n","                               col_dim=1)\n","\n","## calculate basic attention for the token encodings\n","selfAttention(encodings_matrix)"]},{"cell_type":"markdown","id":"1153eec7-5573-41f7-b7de-3106b2ca07f9","metadata":{"id":"1153eec7-5573-41f7-b7de-3106b2ca07f9"},"source":["----"]},{"cell_type":"markdown","id":"c25086ad-92d0-4396-8484-d9dab376ef50","metadata":{"id":"c25086ad-92d0-4396-8484-d9dab376ef50"},"source":["# Print Out Weights and Verify Calculations\n","<a id=\"validate\"></a>"]},{"cell_type":"code","execution_count":null,"id":"0ceddcba-be60-46fe-80bd-fa74e97ea62f","metadata":{"height":47,"colab":{"base_uri":"https://localhost:8080/"},"id":"0ceddcba-be60-46fe-80bd-fa74e97ea62f","executionInfo":{"status":"ok","timestamp":1740168490462,"user_tz":0,"elapsed":12,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"42790a35-f868-4688-ebb3-b099736ca6e1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.5406, -0.1657],\n","        [ 0.5869,  0.6496]], grad_fn=<TransposeBackward0>)"]},"metadata":{},"execution_count":5}],"source":["## print out the weight matrix that creates the queries\n","selfAttention.W_q.weight.transpose(0, 1)"]},{"cell_type":"code","execution_count":null,"id":"03331fba-9985-48e0-92ba-d72118c7a9c0","metadata":{"height":47,"colab":{"base_uri":"https://localhost:8080/"},"id":"03331fba-9985-48e0-92ba-d72118c7a9c0","executionInfo":{"status":"ok","timestamp":1740168514721,"user_tz":0,"elapsed":5,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"3c1cab95-8df6-4898-ed87-1fe4f68dc0a9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1549, -0.3443],\n","        [ 0.1427,  0.4153]], grad_fn=<TransposeBackward0>)"]},"metadata":{},"execution_count":6}],"source":["## print out the weight matrix that creates the keys\n","selfAttention.W_k.weight.transpose(0, 1)"]},{"cell_type":"code","execution_count":null,"id":"89d1e184-4fd0-4e6c-bd24-6916df2f76d8","metadata":{"height":47,"colab":{"base_uri":"https://localhost:8080/"},"id":"89d1e184-4fd0-4e6c-bd24-6916df2f76d8","executionInfo":{"status":"ok","timestamp":1740168516771,"user_tz":0,"elapsed":5,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"454192cb-44f4-43f5-a3b0-41de423badb5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.6233,  0.6146],\n","        [-0.5188,  0.1323]], grad_fn=<TransposeBackward0>)"]},"metadata":{},"execution_count":7}],"source":["## print out the weight matrix that creates the values\n","selfAttention.W_v.weight.transpose(0, 1)"]},{"cell_type":"code","execution_count":null,"id":"55d27388-5b15-4ee5-b39c-61903df18286","metadata":{"height":47,"colab":{"base_uri":"https://localhost:8080/"},"id":"55d27388-5b15-4ee5-b39c-61903df18286","executionInfo":{"status":"ok","timestamp":1740168534921,"user_tz":0,"elapsed":5,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"349a1f62-cecb-4c4a-c8d4-f309e32eaa8c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.7621, -0.0428],\n","        [ 1.1063,  0.7890],\n","        [ 1.1164, -2.1336]], grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":8}],"source":["## calculate the queries\n","selfAttention.W_q(encodings_matrix)"]},{"cell_type":"code","execution_count":null,"id":"c4b3425e-7b2f-4282-b4df-a2da6712c1c6","metadata":{"height":47,"colab":{"base_uri":"https://localhost:8080/"},"id":"c4b3425e-7b2f-4282-b4df-a2da6712c1c6","executionInfo":{"status":"ok","timestamp":1740168537401,"user_tz":0,"elapsed":6,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"08d08eb4-a5fe-4435-f5b1-7156815ee050"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1469, -0.3038],\n","        [ 0.1057,  0.3685],\n","        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":9}],"source":["## calculate the keys\n","selfAttention.W_k(encodings_matrix)"]},{"cell_type":"code","execution_count":null,"id":"9ff03c5b-62a6-4cda-94fc-1aac994e4ad5","metadata":{"height":47,"colab":{"base_uri":"https://localhost:8080/"},"id":"9ff03c5b-62a6-4cda-94fc-1aac994e4ad5","executionInfo":{"status":"ok","timestamp":1740168539735,"user_tz":0,"elapsed":3,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"b71e675c-0ba5-4972-ff60-fe2dbb651891"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.6038,  0.7434],\n","        [-0.3502,  0.5303],\n","        [ 3.8695,  2.4246]], grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":10}],"source":["## calculate the values\n","selfAttention.W_v(encodings_matrix)"]},{"cell_type":"code","execution_count":null,"id":"7d88e411-84ac-4759-abcf-512cf8ff0670","metadata":{"height":47,"colab":{"base_uri":"https://localhost:8080/"},"id":"7d88e411-84ac-4759-abcf-512cf8ff0670","executionInfo":{"status":"ok","timestamp":1740168583675,"user_tz":0,"elapsed":44,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"eaa86d93-8906-4c27-f0ec-40004db92779"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.7621, -0.0428],\n","        [ 1.1063,  0.7890],\n","        [ 1.1164, -2.1336]], grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":11}],"source":["q = selfAttention.W_q(encodings_matrix)\n","q"]},{"cell_type":"code","execution_count":null,"id":"95e191e2-fab3-4290-8085-b64b436dc5e7","metadata":{"height":47,"colab":{"base_uri":"https://localhost:8080/"},"id":"95e191e2-fab3-4290-8085-b64b436dc5e7","executionInfo":{"status":"ok","timestamp":1740168595392,"user_tz":0,"elapsed":6,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"9639af70-e1ba-4985-f873-7c3411b25108"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1469, -0.3038],\n","        [ 0.1057,  0.3685],\n","        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":12}],"source":["k = selfAttention.W_k(encodings_matrix)\n","k"]},{"cell_type":"markdown","source":[],"metadata":{"id":"nHemTH-nAZeW"},"id":"nHemTH-nAZeW"},{"cell_type":"code","execution_count":null,"id":"b4f55998-e89b-40b6-9021-f764ba32a3b5","metadata":{"height":47,"colab":{"base_uri":"https://localhost:8080/"},"id":"b4f55998-e89b-40b6-9021-f764ba32a3b5","executionInfo":{"status":"ok","timestamp":1740168617693,"user_tz":0,"elapsed":6,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"f6e54ead-d475-4a66-888a-46de2fb83a0a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0990,  0.0648, -0.6523],\n","        [-0.4022,  0.4078, -3.0024],\n","        [ 0.4842, -0.6683,  4.0461]], grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":13}],"source":["sims = torch.matmul(q, k.transpose(dim0=0, dim1=1))\n","sims"]},{"cell_type":"code","execution_count":null,"id":"84693537-1874-4698-963a-75c9075b4bb7","metadata":{"height":47,"colab":{"base_uri":"https://localhost:8080/"},"id":"84693537-1874-4698-963a-75c9075b4bb7","executionInfo":{"status":"ok","timestamp":1740168625424,"user_tz":0,"elapsed":20,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"049965c5-cb39-4365-d071-785d0a58e82f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0700,  0.0458, -0.4612],\n","        [-0.2844,  0.2883, -2.1230],\n","        [ 0.3424, -0.4725,  2.8610]], grad_fn=<DivBackward0>)"]},"metadata":{},"execution_count":14}],"source":["scaled_sims = sims / (torch.tensor(2)**0.5)\n","scaled_sims"]},{"cell_type":"code","execution_count":null,"id":"9bbc718e-c73c-4713-8b29-73962e39645d","metadata":{"height":47,"colab":{"base_uri":"https://localhost:8080/"},"id":"9bbc718e-c73c-4713-8b29-73962e39645d","executionInfo":{"status":"ok","timestamp":1740168631146,"user_tz":0,"elapsed":6,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"efccc96c-e09b-44b5-93dc-b7197bc97355"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.3573, 0.4011, 0.2416],\n","        [0.3410, 0.6047, 0.0542],\n","        [0.0722, 0.0320, 0.8959]], grad_fn=<SoftmaxBackward0>)"]},"metadata":{},"execution_count":15}],"source":["attention_percents = F.softmax(scaled_sims, dim=1)\n","attention_percents"]},{"cell_type":"code","execution_count":null,"id":"8c2edce0-9720-41ba-8916-e3415ef05e81","metadata":{"height":30,"colab":{"base_uri":"https://localhost:8080/"},"id":"8c2edce0-9720-41ba-8916-e3415ef05e81","executionInfo":{"status":"ok","timestamp":1740168672169,"user_tz":0,"elapsed":42,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"d810490b-a389-48fd-d482-a7cb8c7cdd22"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0100, 1.0641],\n","        [0.2040, 0.7057],\n","        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":16}],"source":["torch.matmul(attention_percents, selfAttention.W_v(encodings_matrix))"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}