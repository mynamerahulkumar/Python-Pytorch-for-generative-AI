{"cells":[{"cell_type":"markdown","id":"9f40c232-df6e-49df-9016-6459e4af2e1e","metadata":{"tags":[],"id":"9f40c232-df6e-49df-9016-6459e4af2e1e"},"source":["# Coding Masked Self-Attention in PyTroch!!!\n","\n"]},{"cell_type":"markdown","id":"855d3a52-a43e-44cf-b8b6-b2ce88bea382","metadata":{"id":"855d3a52-a43e-44cf-b8b6-b2ce88bea382"},"source":["----"]},{"cell_type":"markdown","id":"63b86036-1369-441c-a14d-3ba7bdaa103b","metadata":{"tags":[],"id":"63b86036-1369-441c-a14d-3ba7bdaa103b"},"source":["# Import the modules that will do all the work"]},{"cell_type":"code","execution_count":6,"id":"5c520e0b-c6e4-43ce-93f5-c0f2b5e75438","metadata":{"height":79,"id":"5c520e0b-c6e4-43ce-93f5-c0f2b5e75438","executionInfo":{"status":"ok","timestamp":1740778837916,"user_tz":0,"elapsed":6,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}}},"outputs":[],"source":["import torch ## torch let's us create tensors and also provides helper functions\n","import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n","import torch.nn.functional as F # This gives us the softmax()"]},{"cell_type":"markdown","id":"6e58ebb2-1798-41c3-9ff8-1b4f50605964","metadata":{"id":"6e58ebb2-1798-41c3-9ff8-1b4f50605964"},"source":["----"]},{"cell_type":"markdown","id":"125c4f0d-83b6-488f-8d91-66c54776bf2f","metadata":{"id":"125c4f0d-83b6-488f-8d91-66c54776bf2f"},"source":["# Code Masked Self-Attention\n","<a id=\"masked\"></a>"]},{"cell_type":"code","execution_count":8,"id":"e3392130-cd25-4000-97bb-9612764c83a8","metadata":{"height":691,"id":"e3392130-cd25-4000-97bb-9612764c83a8","executionInfo":{"status":"ok","timestamp":1740778955261,"user_tz":0,"elapsed":2,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}}},"outputs":[],"source":["class MaskedSelfAttention(nn.Module):\n","\n","    def __init__(self, d_model=2,\n","                 row_dim=0,\n","                 col_dim=1):\n","\n","        super().__init__()\n","\n","        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n","        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n","        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n","\n","        self.row_dim = row_dim\n","        self.col_dim = col_dim\n","\n","\n","    def forward(self, token_encodings, mask=None):\n","\n","        q = self.W_q(token_encodings)\n","        k = self.W_k(token_encodings)\n","        v = self.W_v(token_encodings)\n","\n","        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n","\n","        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n","\n","        if mask is not None:\n","            ## Here we are masking out things we don't want to pay attention to\n","            ##\n","            ## We replace values we wanted masked out\n","            ## with a very small negative number so that the SoftMax() function\n","            ## will give all masked elements an output value (or \"probability\") of 0.\n","            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9) # I've also seen -1e20 and -9e15 used in masking\n","\n","        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n","\n","        attention_scores = torch.matmul(attention_percents, v)\n","\n","        return attention_scores"]},{"cell_type":"markdown","id":"371cd1e0-0216-438d-a241-05533e4b374d","metadata":{"id":"371cd1e0-0216-438d-a241-05533e4b374d"},"source":["----"]},{"cell_type":"markdown","id":"c75aaaa3-29e0-4c03-8a11-816bd4b7aea3","metadata":{"id":"c75aaaa3-29e0-4c03-8a11-816bd4b7aea3"},"source":["# Calculate Masked Self-Attention\n","<a id=\"masked\"></a>"]},{"cell_type":"code","execution_count":10,"id":"0946c6d7-9960-4f49-8aa6-20dd614efbdc","metadata":{"height":334,"colab":{"base_uri":"https://localhost:8080/"},"id":"0946c6d7-9960-4f49-8aa6-20dd614efbdc","executionInfo":{"status":"ok","timestamp":1740779025627,"user_tz":0,"elapsed":9,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"75a9e4cf-4d70-4f5b-ffd2-23bf49adcbe9"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 0.],\n","        [1., 1., 0.],\n","        [1., 1., 1.]])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[False,  True,  True],\n","        [False, False,  True],\n","        [False, False, False]])"]},"metadata":{},"execution_count":10}],"source":["## create a matrix of token encodings...\n","encodings_matrix = torch.tensor([[1.16, 0.23],\n","                                 [0.57, 1.36],\n","                                 [4.41, -2.16]])\n","\n","## set the seed for the random number generator\n","torch.manual_seed(42)\n","\n","## create a masked self-attention object\n","maskedSelfAttention = MaskedSelfAttention(d_model=2,\n","                               row_dim=0,\n","                               col_dim=1)\n","\n","## create the mask so that we don't use\n","## tokens that come after a token of interest\n","mask = torch.tril(torch.ones(3, 3))\n","print(mask)\n","mask = mask == 0\n","mask # print out the mask"]},{"cell_type":"code","execution_count":11,"id":"dbfda93b-f139-4bc4-aa17-a502888ef48c","metadata":{"height":47,"colab":{"base_uri":"https://localhost:8080/"},"id":"dbfda93b-f139-4bc4-aa17-a502888ef48c","executionInfo":{"status":"ok","timestamp":1740779076124,"user_tz":0,"elapsed":59,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"b8ed0188-c2a6-49b3-fe93-f892022431c4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.6038,  0.7434],\n","        [-0.0062,  0.6072],\n","        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":11}],"source":["## calculate masked self-attention\n","maskedSelfAttention(encodings_matrix, mask)"]},{"cell_type":"markdown","id":"1153eec7-5573-41f7-b7de-3106b2ca07f9","metadata":{"id":"1153eec7-5573-41f7-b7de-3106b2ca07f9"},"source":["----"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}