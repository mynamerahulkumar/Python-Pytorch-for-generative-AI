{"cells":[{"cell_type":"markdown","id":"9f40c232-df6e-49df-9016-6459e4af2e1e","metadata":{"tags":[],"id":"9f40c232-df6e-49df-9016-6459e4af2e1e"},"source":["# Encoder Decoder multi head attention Pytorch\n","\n"]},{"cell_type":"markdown","id":"f4226d63-8d76-40bc-a8e6-0f290a159418","metadata":{"id":"f4226d63-8d76-40bc-a8e6-0f290a159418"},"source":["----\n","\n","In this tutorial, we will code a class that is capable of all **3** types of **Attention** that we have studied, **Self-Attention**, **Masked Self-Attention**, and **Encoder-Decoder Attention**. We'll also code a few lines that will make **Multi-Headed Attention** work.\n","\n"]},{"cell_type":"markdown","id":"855d3a52-a43e-44cf-b8b6-b2ce88bea382","metadata":{"id":"855d3a52-a43e-44cf-b8b6-b2ce88bea382"},"source":["----"]},{"cell_type":"markdown","id":"63b86036-1369-441c-a14d-3ba7bdaa103b","metadata":{"tags":[],"id":"63b86036-1369-441c-a14d-3ba7bdaa103b"},"source":["# Import the modules that will do all the work"]},{"cell_type":"code","execution_count":null,"id":"5c520e0b-c6e4-43ce-93f5-c0f2b5e75438","metadata":{"height":79,"id":"5c520e0b-c6e4-43ce-93f5-c0f2b5e75438"},"outputs":[],"source":["import torch ## torch let's us create tensors and also provides helper functions\n","import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n","import torch.nn.functional as F # This gives us the softmax()"]},{"cell_type":"markdown","id":"6e58ebb2-1798-41c3-9ff8-1b4f50605964","metadata":{"id":"6e58ebb2-1798-41c3-9ff8-1b4f50605964"},"source":["----"]},{"cell_type":"markdown","id":"125c4f0d-83b6-488f-8d91-66c54776bf2f","metadata":{"id":"125c4f0d-83b6-488f-8d91-66c54776bf2f"},"source":["# Code Attention\n","<a id=\"attention\"></a>"]},{"cell_type":"code","execution_count":null,"id":"e3392130-cd25-4000-97bb-9612764c83a8","metadata":{"height":640,"id":"e3392130-cd25-4000-97bb-9612764c83a8"},"outputs":[],"source":["class Attention(nn.Module):\n","\n","    def __init__(self, d_model=2,\n","                 row_dim=0,\n","                 col_dim=1):\n","\n","        super().__init__()\n","\n","        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n","        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n","        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n","\n","        self.row_dim = row_dim\n","        self.col_dim = col_dim\n","\n","\n","    ## The only change from SelfAttention and attention is that\n","    ## now we expect 3 sets of encodings to be passed in...\n","    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n","        ## ...and we pass those sets of encodings to the various weight matrices.\n","        q = self.W_q(encodings_for_q)\n","        k = self.W_k(encodings_for_k)\n","        v = self.W_v(encodings_for_v)\n","\n","        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n","\n","        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n","\n","        if mask is not None:\n","            print(\"Mask attention\")\n","            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n","\n","        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n","\n","        attention_scores = torch.matmul(attention_percents, v)\n","\n","        return attention_scores"]},{"cell_type":"markdown","id":"371cd1e0-0216-438d-a241-05533e4b374d","metadata":{"id":"371cd1e0-0216-438d-a241-05533e4b374d"},"source":["----"]},{"cell_type":"markdown","id":"c75aaaa3-29e0-4c03-8a11-816bd4b7aea3","metadata":{"id":"c75aaaa3-29e0-4c03-8a11-816bd4b7aea3"},"source":["# Calculate Encoder-Decoder Attention\n","<a id=\"calculate\"></a>"]},{"cell_type":"code","execution_count":null,"id":"0946c6d7-9960-4f49-8aa6-20dd614efbdc","metadata":{"height":419,"colab":{"base_uri":"https://localhost:8080/"},"id":"0946c6d7-9960-4f49-8aa6-20dd614efbdc","executionInfo":{"status":"ok","timestamp":1740777452474,"user_tz":0,"elapsed":15,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"402a9b70-9f3e-4d96-9958-55d46d9243dc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0100, 1.0641],\n","        [0.2040, 0.7057],\n","        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":15}],"source":["## create matrices of token encodings...\n","encodings_for_q = torch.tensor([[1.16, 0.23],\n","                                [0.57, 1.36],\n","                                [4.41, -2.16]])\n","\n","encodings_for_k = torch.tensor([[1.16, 0.23],\n","                                [0.57, 1.36],\n","                                [4.41, -2.16]])\n","\n","encodings_for_v = torch.tensor([[1.16, 0.23],\n","                                [0.57, 1.36],\n","                                [4.41, -2.16]])\n","\n","## set the seed for the random number generator\n","torch.manual_seed(42)\n","\n","## create an attention object\n","attention = Attention(d_model=2,\n","                      row_dim=0,\n","                      col_dim=1)\n","\n","## calculate encoder-decoder attention\n","attention(encodings_for_q, encodings_for_k, encodings_for_v)"]},{"cell_type":"markdown","id":"1153eec7-5573-41f7-b7de-3106b2ca07f9","metadata":{"id":"1153eec7-5573-41f7-b7de-3106b2ca07f9"},"source":["----"]},{"cell_type":"markdown","id":"c8a7c8c7-1d02-4064-8302-7bcef7c67dc4","metadata":{"id":"c8a7c8c7-1d02-4064-8302-7bcef7c67dc4"},"source":["# Code Mutli-Head Attention\n","<a id=\"multi\"></a>"]},{"cell_type":"code","execution_count":null,"id":"36d5a6a6-3348-40c3-ac6a-b90a47b400d3","metadata":{"height":504,"id":"36d5a6a6-3348-40c3-ac6a-b90a47b400d3"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","\n","    def __init__(self,\n","                 d_model=2,\n","                 row_dim=0,\n","                 col_dim=1,\n","                 num_heads=1):\n","\n","        super().__init__()\n","\n","        ## create a bunch of attention heads\n","        self.heads = nn.ModuleList(\n","            [Attention(d_model, row_dim, col_dim)\n","             for _ in range(num_heads)]\n","        )\n","\n","        self.col_dim = col_dim\n","\n","    def forward(self,\n","                encodings_for_q,\n","                encodings_for_k,\n","                encodings_for_v):\n","\n","        ## run the data through all of the attention heads\n","        return torch.cat([head(encodings_for_q,\n","                               encodings_for_k,\n","                               encodings_for_v)\n","                          for head in self.heads], dim=self.col_dim)"]},{"cell_type":"markdown","id":"4a798f50-cf6e-4643-bf88-97adbe81c202","metadata":{"id":"4a798f50-cf6e-4643-bf88-97adbe81c202"},"source":["----"]},{"cell_type":"markdown","id":"8241b0b7-ade3-45f5-985d-7b370d77ec76","metadata":{"id":"8241b0b7-ade3-45f5-985d-7b370d77ec76"},"source":["# Calcualte Multi-Head Attention\n","<a id=\"calcMulti\"></a>"]},{"cell_type":"markdown","id":"e607d714-bdcd-4d6a-a826-8c8c51802e72","metadata":{"id":"e607d714-bdcd-4d6a-a826-8c8c51802e72"},"source":["First, verify that we can still correctly calculate attention with a single head..."]},{"cell_type":"code","execution_count":null,"id":"1b022438-acaa-4fff-8581-748c2151aebb","metadata":{"height":215,"colab":{"base_uri":"https://localhost:8080/"},"id":"1b022438-acaa-4fff-8581-748c2151aebb","executionInfo":{"status":"ok","timestamp":1740777646292,"user_tz":0,"elapsed":6,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"1c91da12-e689-43f5-8a0f-7484f5cf56d4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0100, 1.0641],\n","        [0.2040, 0.7057],\n","        [3.4989, 2.2427]], grad_fn=<CatBackward0>)"]},"metadata":{},"execution_count":18}],"source":["## set the seed for the random number generator\n","torch.manual_seed(42)\n","\n","## create an attention object\n","multiHeadAttention = MultiHeadAttention(d_model=2,\n","                                        row_dim=0,\n","                                        col_dim=1,\n","                                        num_heads=1)\n","\n","## calculate encoder-decoder attention\n","multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"]},{"cell_type":"markdown","id":"bea43a62-94ea-430b-a738-3ab12b4fc6ba","metadata":{"id":"bea43a62-94ea-430b-a738-3ab12b4fc6ba"},"source":["Second, calculate attention with multiple heads..."]},{"cell_type":"code","execution_count":null,"id":"ec152f85","metadata":{"height":215,"colab":{"base_uri":"https://localhost:8080/"},"id":"ec152f85","executionInfo":{"status":"ok","timestamp":1740777654014,"user_tz":0,"elapsed":10,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"63e038f5-b1d0-4315-843b-6bf9044a2c13"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.0100,  1.0641, -0.7081, -0.8268],\n","        [ 0.2040,  0.7057, -0.7417, -0.9193],\n","        [ 3.4989,  2.2427, -0.7190, -0.8447]], grad_fn=<CatBackward0>)"]},"metadata":{},"execution_count":19}],"source":["## set the seed for the random number generator\n","torch.manual_seed(42)\n","\n","## create an attention object\n","multiHeadAttention = MultiHeadAttention(d_model=2,\n","                                        row_dim=0,\n","                                        col_dim=1,\n","                                        num_heads=2)\n","\n","## calculate encoder-decoder attention\n","multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}