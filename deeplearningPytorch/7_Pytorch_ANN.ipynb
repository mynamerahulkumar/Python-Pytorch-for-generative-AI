{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7E6gSJri2IL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content/drive/MyDrive/A_doc_imp/A_Udeyme/A_Python_and_Pytorch_for_generative_AI/Pytorch/data/X.npy"
      ],
      "metadata": {
        "id": "i9uPQcU2Ys-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#device confif\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4IsOY1GYtyF",
        "outputId": "524f1626-d6a7-470f-db4b-22eabcd45ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "input_size=4096 # 64*64 images\n",
        "hidden_size1 = 2048  # First hidden layer\n",
        "hidden_size2 = 1024  # Second hidden layer\n",
        "hidden_size3 = 512   # Third hidden layer\n",
        "hidden_size4 = 256   # Fourth hidden layer\n",
        "num_classes=10\n",
        "num_epoch=500\n",
        "batch_size=50\n",
        "learning_rate=0.001\n"
      ],
      "metadata": {
        "id": "dAJG-2t9Yt08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shOv6-JHYt3b",
        "outputId": "d4364984-9bce-4d6e-b03b-503ad2e30fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.load('/content/drive/MyDrive/A_doc_imp/A_Udeyme/A_Python_and_Pytorch_for_generative_AI/Pytorch/data/X.npy')\n",
        "Y=np.load('/content/drive/MyDrive/A_doc_imp/A_Udeyme/A_Python_and_Pytorch_for_generative_AI/Pytorch/data/Y.npy')\n"
      ],
      "metadata": {
        "id": "2X9UbD_gYt6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuVTxzaCYuCg",
        "outputId": "8c64ae5c-133f-412b-9601-e31fd30d7d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2062, 64, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-X6GokeYuE-",
        "outputId": "20fab093-7c61-4b51-aff2-0500c542cf7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2062, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u68JwKK_YuHc",
        "outputId": "e8bbfa27-1cea-44d1-e99a-ee87a8b2ca24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize and resshape the dataset\n",
        "X=X.reshape(-1,64*64)/255.0"
      ],
      "metadata": {
        "id": "Qchp4DwBYuKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY8JMe-IhA2r",
        "outputId": "e591f599-47b3-48f9-8563-58737b2729c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2062, 4096)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfCueokXK00b",
        "outputId": "855b9d37-f82e-488f-a6d6-26d50d73c645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2062, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=torch.tensor(X,dtype=torch.float32)\n",
        "Y = np.argmax(Y, axis=1)  # Convert one-hot to class indices\n",
        "Y=torch.tensor(Y,dtype=torch.long)\n",
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BzNVOGShA5M",
        "outputId": "b021667a-fbdf-4102-a31d-8ce2e9e5d98b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2062])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Z = np.array([[1, 0, 0],   # Class 0\n",
        "#               [0, 1, 0],   # Class 1\n",
        "#               [0, 0, 1]])  # Class 2\n",
        "\n",
        "# Z = np.argmax(Z, axis=1)  # Converts to class indices\n",
        "# print(Y)  # Output: [0, 1, 2]"
      ],
      "metadata": {
        "id": "AcKasaLmLEXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.15,random_state=42)"
      ],
      "metadata": {
        "id": "EzSk1mrFhA7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crearte custom dataset\n",
        "train_dataset=torch.utils.data.TensorDataset(X_train,Y_train)\n",
        "test_dataset=torch.utils.data.TensorDataset(X_test,Y_test)"
      ],
      "metadata": {
        "id": "wIzWYWQMhA-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)\n"
      ],
      "metadata": {
        "id": "GoKAk1MwhBBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
        "        self.fc4 = nn.Linear(hidden_size3, hidden_size4)\n",
        "        self.fc5 = nn.Linear(hidden_size4, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.leaky_relu(self.fc2(x),negative_slope=0.01)\n",
        "        x = F.elu(self.fc3(x))\n",
        "        x = F.silu(self.fc4(x))\n",
        "        x = self.fc5(x)  # Output layer\n",
        "        return x"
      ],
      "metadata": {
        "id": "n4W7bXLOYuMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=ANN().to(device)"
      ],
      "metadata": {
        "id": "sS4dzYvWYuPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "t-KlrYrvlmm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epoch):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 20 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epoch}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjU_OdaIlmqj",
        "outputId": "46c75f7a-9c18-4346-c26c-f20ed9619e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Step [20/36], Loss: 2.3141\n",
            "Epoch [2/500], Step [20/36], Loss: 2.2998\n",
            "Epoch [3/500], Step [20/36], Loss: 2.3004\n",
            "Epoch [4/500], Step [20/36], Loss: 2.1871\n",
            "Epoch [5/500], Step [20/36], Loss: 2.2459\n",
            "Epoch [6/500], Step [20/36], Loss: 2.0739\n",
            "Epoch [7/500], Step [20/36], Loss: 2.1676\n",
            "Epoch [8/500], Step [20/36], Loss: 2.0043\n",
            "Epoch [9/500], Step [20/36], Loss: 1.9422\n",
            "Epoch [10/500], Step [20/36], Loss: 2.0155\n",
            "Epoch [11/500], Step [20/36], Loss: 1.7451\n",
            "Epoch [12/500], Step [20/36], Loss: 2.0061\n",
            "Epoch [13/500], Step [20/36], Loss: 1.8309\n",
            "Epoch [14/500], Step [20/36], Loss: 1.9879\n",
            "Epoch [15/500], Step [20/36], Loss: 1.7847\n",
            "Epoch [16/500], Step [20/36], Loss: 1.9252\n",
            "Epoch [17/500], Step [20/36], Loss: 1.9585\n",
            "Epoch [18/500], Step [20/36], Loss: 2.1784\n",
            "Epoch [19/500], Step [20/36], Loss: 1.9816\n",
            "Epoch [20/500], Step [20/36], Loss: 2.2567\n",
            "Epoch [21/500], Step [20/36], Loss: 1.9621\n",
            "Epoch [22/500], Step [20/36], Loss: 1.8863\n",
            "Epoch [23/500], Step [20/36], Loss: 1.9092\n",
            "Epoch [24/500], Step [20/36], Loss: 1.9600\n",
            "Epoch [25/500], Step [20/36], Loss: 1.8801\n",
            "Epoch [26/500], Step [20/36], Loss: 1.7017\n",
            "Epoch [27/500], Step [20/36], Loss: 1.8370\n",
            "Epoch [28/500], Step [20/36], Loss: 1.9165\n",
            "Epoch [29/500], Step [20/36], Loss: 2.0021\n",
            "Epoch [30/500], Step [20/36], Loss: 1.9341\n",
            "Epoch [31/500], Step [20/36], Loss: 1.8224\n",
            "Epoch [32/500], Step [20/36], Loss: 1.8642\n",
            "Epoch [33/500], Step [20/36], Loss: 1.6044\n",
            "Epoch [34/500], Step [20/36], Loss: 1.6818\n",
            "Epoch [35/500], Step [20/36], Loss: 1.8835\n",
            "Epoch [36/500], Step [20/36], Loss: 1.8261\n",
            "Epoch [37/500], Step [20/36], Loss: 1.9502\n",
            "Epoch [38/500], Step [20/36], Loss: 1.6703\n",
            "Epoch [39/500], Step [20/36], Loss: 1.6638\n",
            "Epoch [40/500], Step [20/36], Loss: 1.8380\n",
            "Epoch [41/500], Step [20/36], Loss: 1.7841\n",
            "Epoch [42/500], Step [20/36], Loss: 1.5925\n",
            "Epoch [43/500], Step [20/36], Loss: 1.6000\n",
            "Epoch [44/500], Step [20/36], Loss: 1.7847\n",
            "Epoch [45/500], Step [20/36], Loss: 1.5935\n",
            "Epoch [46/500], Step [20/36], Loss: 1.5329\n",
            "Epoch [47/500], Step [20/36], Loss: 1.4805\n",
            "Epoch [48/500], Step [20/36], Loss: 1.5697\n",
            "Epoch [49/500], Step [20/36], Loss: 1.7144\n",
            "Epoch [50/500], Step [20/36], Loss: 1.7218\n",
            "Epoch [51/500], Step [20/36], Loss: 1.5866\n",
            "Epoch [52/500], Step [20/36], Loss: 1.3515\n",
            "Epoch [53/500], Step [20/36], Loss: 1.5460\n",
            "Epoch [54/500], Step [20/36], Loss: 1.3664\n",
            "Epoch [55/500], Step [20/36], Loss: 1.8388\n",
            "Epoch [56/500], Step [20/36], Loss: 1.3281\n",
            "Epoch [57/500], Step [20/36], Loss: 1.3523\n",
            "Epoch [58/500], Step [20/36], Loss: 1.6392\n",
            "Epoch [59/500], Step [20/36], Loss: 1.4474\n",
            "Epoch [60/500], Step [20/36], Loss: 1.7533\n",
            "Epoch [61/500], Step [20/36], Loss: 1.2179\n",
            "Epoch [62/500], Step [20/36], Loss: 1.1904\n",
            "Epoch [63/500], Step [20/36], Loss: 1.1133\n",
            "Epoch [64/500], Step [20/36], Loss: 1.0853\n",
            "Epoch [65/500], Step [20/36], Loss: 1.1644\n",
            "Epoch [66/500], Step [20/36], Loss: 1.4962\n",
            "Epoch [67/500], Step [20/36], Loss: 1.1292\n",
            "Epoch [68/500], Step [20/36], Loss: 1.0997\n",
            "Epoch [69/500], Step [20/36], Loss: 1.2271\n",
            "Epoch [70/500], Step [20/36], Loss: 1.0962\n",
            "Epoch [71/500], Step [20/36], Loss: 1.3663\n",
            "Epoch [72/500], Step [20/36], Loss: 1.0874\n",
            "Epoch [73/500], Step [20/36], Loss: 1.1958\n",
            "Epoch [74/500], Step [20/36], Loss: 1.0634\n",
            "Epoch [75/500], Step [20/36], Loss: 1.1159\n",
            "Epoch [76/500], Step [20/36], Loss: 0.8902\n",
            "Epoch [77/500], Step [20/36], Loss: 1.1083\n",
            "Epoch [78/500], Step [20/36], Loss: 1.0968\n",
            "Epoch [79/500], Step [20/36], Loss: 0.9608\n",
            "Epoch [80/500], Step [20/36], Loss: 1.1041\n",
            "Epoch [81/500], Step [20/36], Loss: 1.1731\n",
            "Epoch [82/500], Step [20/36], Loss: 1.2379\n",
            "Epoch [83/500], Step [20/36], Loss: 0.9568\n",
            "Epoch [84/500], Step [20/36], Loss: 0.9383\n",
            "Epoch [85/500], Step [20/36], Loss: 1.0997\n",
            "Epoch [86/500], Step [20/36], Loss: 1.2465\n",
            "Epoch [87/500], Step [20/36], Loss: 0.7488\n",
            "Epoch [88/500], Step [20/36], Loss: 1.2365\n",
            "Epoch [89/500], Step [20/36], Loss: 0.8400\n",
            "Epoch [90/500], Step [20/36], Loss: 1.0041\n",
            "Epoch [91/500], Step [20/36], Loss: 1.0625\n",
            "Epoch [92/500], Step [20/36], Loss: 0.6166\n",
            "Epoch [93/500], Step [20/36], Loss: 0.7521\n",
            "Epoch [94/500], Step [20/36], Loss: 1.0749\n",
            "Epoch [95/500], Step [20/36], Loss: 1.4011\n",
            "Epoch [96/500], Step [20/36], Loss: 1.1131\n",
            "Epoch [97/500], Step [20/36], Loss: 0.6185\n",
            "Epoch [98/500], Step [20/36], Loss: 0.7838\n",
            "Epoch [99/500], Step [20/36], Loss: 0.7214\n",
            "Epoch [100/500], Step [20/36], Loss: 1.1268\n",
            "Epoch [101/500], Step [20/36], Loss: 1.0398\n",
            "Epoch [102/500], Step [20/36], Loss: 1.4616\n",
            "Epoch [103/500], Step [20/36], Loss: 0.7873\n",
            "Epoch [104/500], Step [20/36], Loss: 0.7092\n",
            "Epoch [105/500], Step [20/36], Loss: 1.0943\n",
            "Epoch [106/500], Step [20/36], Loss: 0.9032\n",
            "Epoch [107/500], Step [20/36], Loss: 0.8653\n",
            "Epoch [108/500], Step [20/36], Loss: 1.3353\n",
            "Epoch [109/500], Step [20/36], Loss: 0.4978\n",
            "Epoch [110/500], Step [20/36], Loss: 1.3837\n",
            "Epoch [111/500], Step [20/36], Loss: 0.7601\n",
            "Epoch [112/500], Step [20/36], Loss: 1.0843\n",
            "Epoch [113/500], Step [20/36], Loss: 1.0718\n",
            "Epoch [114/500], Step [20/36], Loss: 0.8126\n",
            "Epoch [115/500], Step [20/36], Loss: 0.7855\n",
            "Epoch [116/500], Step [20/36], Loss: 0.9213\n",
            "Epoch [117/500], Step [20/36], Loss: 0.7233\n",
            "Epoch [118/500], Step [20/36], Loss: 1.1583\n",
            "Epoch [119/500], Step [20/36], Loss: 0.6859\n",
            "Epoch [120/500], Step [20/36], Loss: 0.8947\n",
            "Epoch [121/500], Step [20/36], Loss: 1.0126\n",
            "Epoch [122/500], Step [20/36], Loss: 0.9515\n",
            "Epoch [123/500], Step [20/36], Loss: 0.7505\n",
            "Epoch [124/500], Step [20/36], Loss: 0.5565\n",
            "Epoch [125/500], Step [20/36], Loss: 0.5716\n",
            "Epoch [126/500], Step [20/36], Loss: 0.7681\n",
            "Epoch [127/500], Step [20/36], Loss: 0.5379\n",
            "Epoch [128/500], Step [20/36], Loss: 0.8588\n",
            "Epoch [129/500], Step [20/36], Loss: 0.9286\n",
            "Epoch [130/500], Step [20/36], Loss: 0.7909\n",
            "Epoch [131/500], Step [20/36], Loss: 0.6223\n",
            "Epoch [132/500], Step [20/36], Loss: 0.5547\n",
            "Epoch [133/500], Step [20/36], Loss: 0.7484\n",
            "Epoch [134/500], Step [20/36], Loss: 0.6610\n",
            "Epoch [135/500], Step [20/36], Loss: 1.3918\n",
            "Epoch [136/500], Step [20/36], Loss: 0.7835\n",
            "Epoch [137/500], Step [20/36], Loss: 0.6298\n",
            "Epoch [138/500], Step [20/36], Loss: 0.6594\n",
            "Epoch [139/500], Step [20/36], Loss: 0.5845\n",
            "Epoch [140/500], Step [20/36], Loss: 0.6140\n",
            "Epoch [141/500], Step [20/36], Loss: 0.7206\n",
            "Epoch [142/500], Step [20/36], Loss: 0.7861\n",
            "Epoch [143/500], Step [20/36], Loss: 0.6013\n",
            "Epoch [144/500], Step [20/36], Loss: 0.3868\n",
            "Epoch [145/500], Step [20/36], Loss: 0.4678\n",
            "Epoch [146/500], Step [20/36], Loss: 0.5673\n",
            "Epoch [147/500], Step [20/36], Loss: 0.5485\n",
            "Epoch [148/500], Step [20/36], Loss: 0.5343\n",
            "Epoch [149/500], Step [20/36], Loss: 0.5720\n",
            "Epoch [150/500], Step [20/36], Loss: 0.4902\n",
            "Epoch [151/500], Step [20/36], Loss: 0.5386\n",
            "Epoch [152/500], Step [20/36], Loss: 0.6557\n",
            "Epoch [153/500], Step [20/36], Loss: 0.5452\n",
            "Epoch [154/500], Step [20/36], Loss: 0.7348\n",
            "Epoch [155/500], Step [20/36], Loss: 0.6346\n",
            "Epoch [156/500], Step [20/36], Loss: 0.3978\n",
            "Epoch [157/500], Step [20/36], Loss: 1.1993\n",
            "Epoch [158/500], Step [20/36], Loss: 0.6983\n",
            "Epoch [159/500], Step [20/36], Loss: 1.0633\n",
            "Epoch [160/500], Step [20/36], Loss: 0.5005\n",
            "Epoch [161/500], Step [20/36], Loss: 0.8552\n",
            "Epoch [162/500], Step [20/36], Loss: 0.4263\n",
            "Epoch [163/500], Step [20/36], Loss: 0.6890\n",
            "Epoch [164/500], Step [20/36], Loss: 0.5639\n",
            "Epoch [165/500], Step [20/36], Loss: 0.2941\n",
            "Epoch [166/500], Step [20/36], Loss: 0.3655\n",
            "Epoch [167/500], Step [20/36], Loss: 0.5811\n",
            "Epoch [168/500], Step [20/36], Loss: 0.4180\n",
            "Epoch [169/500], Step [20/36], Loss: 0.5760\n",
            "Epoch [170/500], Step [20/36], Loss: 0.4935\n",
            "Epoch [171/500], Step [20/36], Loss: 0.5597\n",
            "Epoch [172/500], Step [20/36], Loss: 0.4736\n",
            "Epoch [173/500], Step [20/36], Loss: 0.7266\n",
            "Epoch [174/500], Step [20/36], Loss: 0.8068\n",
            "Epoch [175/500], Step [20/36], Loss: 0.6741\n",
            "Epoch [176/500], Step [20/36], Loss: 1.0858\n",
            "Epoch [177/500], Step [20/36], Loss: 0.4191\n",
            "Epoch [178/500], Step [20/36], Loss: 0.6573\n",
            "Epoch [179/500], Step [20/36], Loss: 0.6387\n",
            "Epoch [180/500], Step [20/36], Loss: 0.4799\n",
            "Epoch [181/500], Step [20/36], Loss: 0.5338\n",
            "Epoch [182/500], Step [20/36], Loss: 0.5591\n",
            "Epoch [183/500], Step [20/36], Loss: 0.7927\n",
            "Epoch [184/500], Step [20/36], Loss: 0.5453\n",
            "Epoch [185/500], Step [20/36], Loss: 0.3431\n",
            "Epoch [186/500], Step [20/36], Loss: 0.7556\n",
            "Epoch [187/500], Step [20/36], Loss: 0.6111\n",
            "Epoch [188/500], Step [20/36], Loss: 0.3097\n",
            "Epoch [189/500], Step [20/36], Loss: 0.3210\n",
            "Epoch [190/500], Step [20/36], Loss: 0.4369\n",
            "Epoch [191/500], Step [20/36], Loss: 0.5653\n",
            "Epoch [192/500], Step [20/36], Loss: 0.4995\n",
            "Epoch [193/500], Step [20/36], Loss: 0.5332\n",
            "Epoch [194/500], Step [20/36], Loss: 0.3725\n",
            "Epoch [195/500], Step [20/36], Loss: 0.3228\n",
            "Epoch [196/500], Step [20/36], Loss: 0.4896\n",
            "Epoch [197/500], Step [20/36], Loss: 0.3459\n",
            "Epoch [198/500], Step [20/36], Loss: 0.3129\n",
            "Epoch [199/500], Step [20/36], Loss: 0.5846\n",
            "Epoch [200/500], Step [20/36], Loss: 0.3877\n",
            "Epoch [201/500], Step [20/36], Loss: 0.4986\n",
            "Epoch [202/500], Step [20/36], Loss: 0.3703\n",
            "Epoch [203/500], Step [20/36], Loss: 0.4084\n",
            "Epoch [204/500], Step [20/36], Loss: 0.6810\n",
            "Epoch [205/500], Step [20/36], Loss: 0.7511\n",
            "Epoch [206/500], Step [20/36], Loss: 0.4116\n",
            "Epoch [207/500], Step [20/36], Loss: 0.4773\n",
            "Epoch [208/500], Step [20/36], Loss: 0.3436\n",
            "Epoch [209/500], Step [20/36], Loss: 0.3083\n",
            "Epoch [210/500], Step [20/36], Loss: 0.2841\n",
            "Epoch [211/500], Step [20/36], Loss: 0.4256\n",
            "Epoch [212/500], Step [20/36], Loss: 0.3460\n",
            "Epoch [213/500], Step [20/36], Loss: 0.3538\n",
            "Epoch [214/500], Step [20/36], Loss: 0.2392\n",
            "Epoch [215/500], Step [20/36], Loss: 0.2967\n",
            "Epoch [216/500], Step [20/36], Loss: 0.3133\n",
            "Epoch [217/500], Step [20/36], Loss: 0.4307\n",
            "Epoch [218/500], Step [20/36], Loss: 0.6391\n",
            "Epoch [219/500], Step [20/36], Loss: 0.3561\n",
            "Epoch [220/500], Step [20/36], Loss: 0.3724\n",
            "Epoch [221/500], Step [20/36], Loss: 0.3002\n",
            "Epoch [222/500], Step [20/36], Loss: 0.5159\n",
            "Epoch [223/500], Step [20/36], Loss: 0.3723\n",
            "Epoch [224/500], Step [20/36], Loss: 0.4464\n",
            "Epoch [225/500], Step [20/36], Loss: 0.2188\n",
            "Epoch [226/500], Step [20/36], Loss: 0.1103\n",
            "Epoch [227/500], Step [20/36], Loss: 0.6185\n",
            "Epoch [228/500], Step [20/36], Loss: 0.3978\n",
            "Epoch [229/500], Step [20/36], Loss: 0.5274\n",
            "Epoch [230/500], Step [20/36], Loss: 0.2861\n",
            "Epoch [231/500], Step [20/36], Loss: 0.2535\n",
            "Epoch [232/500], Step [20/36], Loss: 0.4276\n",
            "Epoch [233/500], Step [20/36], Loss: 0.5266\n",
            "Epoch [234/500], Step [20/36], Loss: 0.1742\n",
            "Epoch [235/500], Step [20/36], Loss: 0.1758\n",
            "Epoch [236/500], Step [20/36], Loss: 0.6437\n",
            "Epoch [237/500], Step [20/36], Loss: 0.3728\n",
            "Epoch [238/500], Step [20/36], Loss: 0.2320\n",
            "Epoch [239/500], Step [20/36], Loss: 0.3244\n",
            "Epoch [240/500], Step [20/36], Loss: 0.2663\n",
            "Epoch [241/500], Step [20/36], Loss: 0.3347\n",
            "Epoch [242/500], Step [20/36], Loss: 0.2458\n",
            "Epoch [243/500], Step [20/36], Loss: 0.2980\n",
            "Epoch [244/500], Step [20/36], Loss: 0.2970\n",
            "Epoch [245/500], Step [20/36], Loss: 0.4147\n",
            "Epoch [246/500], Step [20/36], Loss: 0.4603\n",
            "Epoch [247/500], Step [20/36], Loss: 0.3014\n",
            "Epoch [248/500], Step [20/36], Loss: 0.3165\n",
            "Epoch [249/500], Step [20/36], Loss: 0.1814\n",
            "Epoch [250/500], Step [20/36], Loss: 0.3229\n",
            "Epoch [251/500], Step [20/36], Loss: 0.4746\n",
            "Epoch [252/500], Step [20/36], Loss: 0.2848\n",
            "Epoch [253/500], Step [20/36], Loss: 0.5394\n",
            "Epoch [254/500], Step [20/36], Loss: 0.5499\n",
            "Epoch [255/500], Step [20/36], Loss: 0.5004\n",
            "Epoch [256/500], Step [20/36], Loss: 0.3305\n",
            "Epoch [257/500], Step [20/36], Loss: 0.2331\n",
            "Epoch [258/500], Step [20/36], Loss: 0.3203\n",
            "Epoch [259/500], Step [20/36], Loss: 0.3310\n",
            "Epoch [260/500], Step [20/36], Loss: 0.3698\n",
            "Epoch [261/500], Step [20/36], Loss: 0.3270\n",
            "Epoch [262/500], Step [20/36], Loss: 0.2517\n",
            "Epoch [263/500], Step [20/36], Loss: 0.4188\n",
            "Epoch [264/500], Step [20/36], Loss: 0.1381\n",
            "Epoch [265/500], Step [20/36], Loss: 0.3141\n",
            "Epoch [266/500], Step [20/36], Loss: 0.1845\n",
            "Epoch [267/500], Step [20/36], Loss: 0.1832\n",
            "Epoch [268/500], Step [20/36], Loss: 0.4037\n",
            "Epoch [269/500], Step [20/36], Loss: 0.2460\n",
            "Epoch [270/500], Step [20/36], Loss: 0.6524\n",
            "Epoch [271/500], Step [20/36], Loss: 0.3160\n",
            "Epoch [272/500], Step [20/36], Loss: 0.3555\n",
            "Epoch [273/500], Step [20/36], Loss: 0.5004\n",
            "Epoch [274/500], Step [20/36], Loss: 0.3624\n",
            "Epoch [275/500], Step [20/36], Loss: 0.3461\n",
            "Epoch [276/500], Step [20/36], Loss: 0.2257\n",
            "Epoch [277/500], Step [20/36], Loss: 0.3102\n",
            "Epoch [278/500], Step [20/36], Loss: 0.3743\n",
            "Epoch [279/500], Step [20/36], Loss: 0.3168\n",
            "Epoch [280/500], Step [20/36], Loss: 0.4860\n",
            "Epoch [281/500], Step [20/36], Loss: 0.3222\n",
            "Epoch [282/500], Step [20/36], Loss: 0.2690\n",
            "Epoch [283/500], Step [20/36], Loss: 0.2458\n",
            "Epoch [284/500], Step [20/36], Loss: 0.3082\n",
            "Epoch [285/500], Step [20/36], Loss: 0.2815\n",
            "Epoch [286/500], Step [20/36], Loss: 0.2420\n",
            "Epoch [287/500], Step [20/36], Loss: 0.1662\n",
            "Epoch [288/500], Step [20/36], Loss: 0.0715\n",
            "Epoch [289/500], Step [20/36], Loss: 0.3079\n",
            "Epoch [290/500], Step [20/36], Loss: 1.6766\n",
            "Epoch [291/500], Step [20/36], Loss: 0.3245\n",
            "Epoch [292/500], Step [20/36], Loss: 0.3726\n",
            "Epoch [293/500], Step [20/36], Loss: 0.2311\n",
            "Epoch [294/500], Step [20/36], Loss: 0.1236\n",
            "Epoch [295/500], Step [20/36], Loss: 0.2410\n",
            "Epoch [296/500], Step [20/36], Loss: 0.3053\n",
            "Epoch [297/500], Step [20/36], Loss: 0.2143\n",
            "Epoch [298/500], Step [20/36], Loss: 0.2690\n",
            "Epoch [299/500], Step [20/36], Loss: 0.2931\n",
            "Epoch [300/500], Step [20/36], Loss: 0.6057\n",
            "Epoch [301/500], Step [20/36], Loss: 0.6835\n",
            "Epoch [302/500], Step [20/36], Loss: 0.4115\n",
            "Epoch [303/500], Step [20/36], Loss: 0.3147\n",
            "Epoch [304/500], Step [20/36], Loss: 0.0985\n",
            "Epoch [305/500], Step [20/36], Loss: 0.3045\n",
            "Epoch [306/500], Step [20/36], Loss: 0.2873\n",
            "Epoch [307/500], Step [20/36], Loss: 0.3694\n",
            "Epoch [308/500], Step [20/36], Loss: 0.1043\n",
            "Epoch [309/500], Step [20/36], Loss: 0.2317\n",
            "Epoch [310/500], Step [20/36], Loss: 0.2759\n",
            "Epoch [311/500], Step [20/36], Loss: 0.1417\n",
            "Epoch [312/500], Step [20/36], Loss: 0.1726\n",
            "Epoch [313/500], Step [20/36], Loss: 0.6104\n",
            "Epoch [314/500], Step [20/36], Loss: 0.5531\n",
            "Epoch [315/500], Step [20/36], Loss: 0.3841\n",
            "Epoch [316/500], Step [20/36], Loss: 0.1683\n",
            "Epoch [317/500], Step [20/36], Loss: 0.1567\n",
            "Epoch [318/500], Step [20/36], Loss: 0.1956\n",
            "Epoch [319/500], Step [20/36], Loss: 0.1452\n",
            "Epoch [320/500], Step [20/36], Loss: 0.1355\n",
            "Epoch [321/500], Step [20/36], Loss: 0.5512\n",
            "Epoch [322/500], Step [20/36], Loss: 0.2059\n",
            "Epoch [323/500], Step [20/36], Loss: 0.1972\n",
            "Epoch [324/500], Step [20/36], Loss: 0.2108\n",
            "Epoch [325/500], Step [20/36], Loss: 0.1836\n",
            "Epoch [326/500], Step [20/36], Loss: 0.2838\n",
            "Epoch [327/500], Step [20/36], Loss: 0.0953\n",
            "Epoch [328/500], Step [20/36], Loss: 0.3069\n",
            "Epoch [329/500], Step [20/36], Loss: 0.0958\n",
            "Epoch [330/500], Step [20/36], Loss: 0.1232\n",
            "Epoch [331/500], Step [20/36], Loss: 0.6077\n",
            "Epoch [332/500], Step [20/36], Loss: 0.2290\n",
            "Epoch [333/500], Step [20/36], Loss: 0.2375\n",
            "Epoch [334/500], Step [20/36], Loss: 0.1334\n",
            "Epoch [335/500], Step [20/36], Loss: 0.2103\n",
            "Epoch [336/500], Step [20/36], Loss: 0.3992\n",
            "Epoch [337/500], Step [20/36], Loss: 0.1468\n",
            "Epoch [338/500], Step [20/36], Loss: 0.1849\n",
            "Epoch [339/500], Step [20/36], Loss: 0.1033\n",
            "Epoch [340/500], Step [20/36], Loss: 0.9325\n",
            "Epoch [341/500], Step [20/36], Loss: 0.1480\n",
            "Epoch [342/500], Step [20/36], Loss: 0.1668\n",
            "Epoch [343/500], Step [20/36], Loss: 0.0974\n",
            "Epoch [344/500], Step [20/36], Loss: 0.2255\n",
            "Epoch [345/500], Step [20/36], Loss: 0.1538\n",
            "Epoch [346/500], Step [20/36], Loss: 0.1189\n",
            "Epoch [347/500], Step [20/36], Loss: 0.1263\n",
            "Epoch [348/500], Step [20/36], Loss: 0.1002\n",
            "Epoch [349/500], Step [20/36], Loss: 0.1337\n",
            "Epoch [350/500], Step [20/36], Loss: 0.3607\n",
            "Epoch [351/500], Step [20/36], Loss: 0.2348\n",
            "Epoch [352/500], Step [20/36], Loss: 0.1042\n",
            "Epoch [353/500], Step [20/36], Loss: 0.2632\n",
            "Epoch [354/500], Step [20/36], Loss: 0.0647\n",
            "Epoch [355/500], Step [20/36], Loss: 0.1200\n",
            "Epoch [356/500], Step [20/36], Loss: 0.4462\n",
            "Epoch [357/500], Step [20/36], Loss: 0.4423\n",
            "Epoch [358/500], Step [20/36], Loss: 0.3173\n",
            "Epoch [359/500], Step [20/36], Loss: 0.1683\n",
            "Epoch [360/500], Step [20/36], Loss: 0.1392\n",
            "Epoch [361/500], Step [20/36], Loss: 0.2697\n",
            "Epoch [362/500], Step [20/36], Loss: 0.1460\n",
            "Epoch [363/500], Step [20/36], Loss: 0.0836\n",
            "Epoch [364/500], Step [20/36], Loss: 0.1727\n",
            "Epoch [365/500], Step [20/36], Loss: 0.1913\n",
            "Epoch [366/500], Step [20/36], Loss: 0.3766\n",
            "Epoch [367/500], Step [20/36], Loss: 0.0485\n",
            "Epoch [368/500], Step [20/36], Loss: 0.1635\n",
            "Epoch [369/500], Step [20/36], Loss: 0.2426\n",
            "Epoch [370/500], Step [20/36], Loss: 0.1659\n",
            "Epoch [371/500], Step [20/36], Loss: 0.1361\n",
            "Epoch [372/500], Step [20/36], Loss: 0.4144\n",
            "Epoch [373/500], Step [20/36], Loss: 0.1520\n",
            "Epoch [374/500], Step [20/36], Loss: 0.3765\n",
            "Epoch [375/500], Step [20/36], Loss: 0.1597\n",
            "Epoch [376/500], Step [20/36], Loss: 0.1597\n",
            "Epoch [377/500], Step [20/36], Loss: 0.2521\n",
            "Epoch [378/500], Step [20/36], Loss: 0.2563\n",
            "Epoch [379/500], Step [20/36], Loss: 0.1474\n",
            "Epoch [380/500], Step [20/36], Loss: 0.0556\n",
            "Epoch [381/500], Step [20/36], Loss: 0.0955\n",
            "Epoch [382/500], Step [20/36], Loss: 0.0974\n",
            "Epoch [383/500], Step [20/36], Loss: 0.0479\n",
            "Epoch [384/500], Step [20/36], Loss: 0.0451\n",
            "Epoch [385/500], Step [20/36], Loss: 0.1168\n",
            "Epoch [386/500], Step [20/36], Loss: 0.0908\n",
            "Epoch [387/500], Step [20/36], Loss: 0.6375\n",
            "Epoch [388/500], Step [20/36], Loss: 0.8807\n",
            "Epoch [389/500], Step [20/36], Loss: 0.0869\n",
            "Epoch [390/500], Step [20/36], Loss: 0.1174\n",
            "Epoch [391/500], Step [20/36], Loss: 0.0647\n",
            "Epoch [392/500], Step [20/36], Loss: 0.1126\n",
            "Epoch [393/500], Step [20/36], Loss: 0.1511\n",
            "Epoch [394/500], Step [20/36], Loss: 0.1442\n",
            "Epoch [395/500], Step [20/36], Loss: 0.0248\n",
            "Epoch [396/500], Step [20/36], Loss: 0.0859\n",
            "Epoch [397/500], Step [20/36], Loss: 0.1310\n",
            "Epoch [398/500], Step [20/36], Loss: 0.0675\n",
            "Epoch [399/500], Step [20/36], Loss: 0.0488\n",
            "Epoch [400/500], Step [20/36], Loss: 0.1505\n",
            "Epoch [401/500], Step [20/36], Loss: 0.0474\n",
            "Epoch [402/500], Step [20/36], Loss: 0.6317\n",
            "Epoch [403/500], Step [20/36], Loss: 0.1327\n",
            "Epoch [404/500], Step [20/36], Loss: 0.1069\n",
            "Epoch [405/500], Step [20/36], Loss: 0.1642\n",
            "Epoch [406/500], Step [20/36], Loss: 0.0954\n",
            "Epoch [407/500], Step [20/36], Loss: 0.0874\n",
            "Epoch [408/500], Step [20/36], Loss: 0.0247\n",
            "Epoch [409/500], Step [20/36], Loss: 0.0405\n",
            "Epoch [410/500], Step [20/36], Loss: 0.0365\n",
            "Epoch [411/500], Step [20/36], Loss: 0.0733\n",
            "Epoch [412/500], Step [20/36], Loss: 0.5474\n",
            "Epoch [413/500], Step [20/36], Loss: 0.1928\n",
            "Epoch [414/500], Step [20/36], Loss: 0.0261\n",
            "Epoch [415/500], Step [20/36], Loss: 0.2377\n",
            "Epoch [416/500], Step [20/36], Loss: 0.1026\n",
            "Epoch [417/500], Step [20/36], Loss: 0.0547\n",
            "Epoch [418/500], Step [20/36], Loss: 0.0473\n",
            "Epoch [419/500], Step [20/36], Loss: 0.0376\n",
            "Epoch [420/500], Step [20/36], Loss: 0.1810\n",
            "Epoch [421/500], Step [20/36], Loss: 0.0775\n",
            "Epoch [422/500], Step [20/36], Loss: 0.2310\n",
            "Epoch [423/500], Step [20/36], Loss: 0.1391\n",
            "Epoch [424/500], Step [20/36], Loss: 0.0740\n",
            "Epoch [425/500], Step [20/36], Loss: 0.0931\n",
            "Epoch [426/500], Step [20/36], Loss: 0.0774\n",
            "Epoch [427/500], Step [20/36], Loss: 0.1114\n",
            "Epoch [428/500], Step [20/36], Loss: 0.0379\n",
            "Epoch [429/500], Step [20/36], Loss: 0.0458\n",
            "Epoch [430/500], Step [20/36], Loss: 0.0578\n",
            "Epoch [431/500], Step [20/36], Loss: 0.1541\n",
            "Epoch [432/500], Step [20/36], Loss: 0.1438\n",
            "Epoch [433/500], Step [20/36], Loss: 0.0795\n",
            "Epoch [434/500], Step [20/36], Loss: 0.0228\n",
            "Epoch [435/500], Step [20/36], Loss: 0.0511\n",
            "Epoch [436/500], Step [20/36], Loss: 0.1353\n",
            "Epoch [437/500], Step [20/36], Loss: 0.2007\n",
            "Epoch [438/500], Step [20/36], Loss: 0.4912\n",
            "Epoch [439/500], Step [20/36], Loss: 0.4845\n",
            "Epoch [440/500], Step [20/36], Loss: 0.0865\n",
            "Epoch [441/500], Step [20/36], Loss: 0.0762\n",
            "Epoch [442/500], Step [20/36], Loss: 0.0659\n",
            "Epoch [443/500], Step [20/36], Loss: 0.2144\n",
            "Epoch [444/500], Step [20/36], Loss: 0.0688\n",
            "Epoch [445/500], Step [20/36], Loss: 0.2075\n",
            "Epoch [446/500], Step [20/36], Loss: 0.0377\n",
            "Epoch [447/500], Step [20/36], Loss: 0.0246\n",
            "Epoch [448/500], Step [20/36], Loss: 0.1725\n",
            "Epoch [449/500], Step [20/36], Loss: 0.0685\n",
            "Epoch [450/500], Step [20/36], Loss: 0.2332\n",
            "Epoch [451/500], Step [20/36], Loss: 0.1609\n",
            "Epoch [452/500], Step [20/36], Loss: 1.4737\n",
            "Epoch [453/500], Step [20/36], Loss: 0.6868\n",
            "Epoch [454/500], Step [20/36], Loss: 0.3633\n",
            "Epoch [455/500], Step [20/36], Loss: 0.1975\n",
            "Epoch [456/500], Step [20/36], Loss: 0.2245\n",
            "Epoch [457/500], Step [20/36], Loss: 0.0842\n",
            "Epoch [458/500], Step [20/36], Loss: 0.1512\n",
            "Epoch [459/500], Step [20/36], Loss: 0.0476\n",
            "Epoch [460/500], Step [20/36], Loss: 0.2167\n",
            "Epoch [461/500], Step [20/36], Loss: 0.1601\n",
            "Epoch [462/500], Step [20/36], Loss: 0.1535\n",
            "Epoch [463/500], Step [20/36], Loss: 0.0599\n",
            "Epoch [464/500], Step [20/36], Loss: 0.3832\n",
            "Epoch [465/500], Step [20/36], Loss: 0.1379\n",
            "Epoch [466/500], Step [20/36], Loss: 0.4219\n",
            "Epoch [467/500], Step [20/36], Loss: 0.1514\n",
            "Epoch [468/500], Step [20/36], Loss: 0.0804\n",
            "Epoch [469/500], Step [20/36], Loss: 0.1033\n",
            "Epoch [470/500], Step [20/36], Loss: 0.1324\n",
            "Epoch [471/500], Step [20/36], Loss: 0.1982\n",
            "Epoch [472/500], Step [20/36], Loss: 0.1308\n",
            "Epoch [473/500], Step [20/36], Loss: 0.1523\n",
            "Epoch [474/500], Step [20/36], Loss: 0.0569\n",
            "Epoch [475/500], Step [20/36], Loss: 0.0824\n",
            "Epoch [476/500], Step [20/36], Loss: 0.1857\n",
            "Epoch [477/500], Step [20/36], Loss: 0.2858\n",
            "Epoch [478/500], Step [20/36], Loss: 0.6476\n",
            "Epoch [479/500], Step [20/36], Loss: 0.1164\n",
            "Epoch [480/500], Step [20/36], Loss: 0.0920\n",
            "Epoch [481/500], Step [20/36], Loss: 0.1620\n",
            "Epoch [482/500], Step [20/36], Loss: 0.1945\n",
            "Epoch [483/500], Step [20/36], Loss: 0.0704\n",
            "Epoch [484/500], Step [20/36], Loss: 0.0367\n",
            "Epoch [485/500], Step [20/36], Loss: 0.0316\n",
            "Epoch [486/500], Step [20/36], Loss: 0.1093\n",
            "Epoch [487/500], Step [20/36], Loss: 0.2132\n",
            "Epoch [488/500], Step [20/36], Loss: 0.2011\n",
            "Epoch [489/500], Step [20/36], Loss: 0.1580\n",
            "Epoch [490/500], Step [20/36], Loss: 0.0554\n",
            "Epoch [491/500], Step [20/36], Loss: 0.0760\n",
            "Epoch [492/500], Step [20/36], Loss: 0.1085\n",
            "Epoch [493/500], Step [20/36], Loss: 0.0927\n",
            "Epoch [494/500], Step [20/36], Loss: 0.1459\n",
            "Epoch [495/500], Step [20/36], Loss: 0.0795\n",
            "Epoch [496/500], Step [20/36], Loss: 0.0934\n",
            "Epoch [497/500], Step [20/36], Loss: 0.0963\n",
            "Epoch [498/500], Step [20/36], Loss: 0.0803\n",
            "Epoch [499/500], Step [20/36], Loss: 0.3416\n",
            "Epoch [500/500], Step [20/36], Loss: 0.2151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  correct=0\n",
        "  total=0\n",
        "  for images,labels in test_loader:\n",
        "    images,labels=images.to(device),labels.to(device)\n",
        "    outputs=model(images)\n",
        "    _,predicted=torch.max(outputs.data,1)\n",
        "    total+=labels.size(0)\n",
        "    correct+=(predicted==labels).sum().item()\n",
        "print(f'Accuracy on test data{100*correct/total:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZVlr6tjlmt-",
        "outputId": "6a8bd507-8847-452b-8251-a0baef553674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data78.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training the data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  correct=0\n",
        "  total=0\n",
        "  for images,labels in train_loader:\n",
        "    images,labels=images.to(device),labels.to(device)\n",
        "    outputs=model(images)\n",
        "    _,predicted=torch.max(outputs.data,1)\n",
        "    total+=labels.size(0)\n",
        "    correct+=(predicted==labels).sum().item()\n",
        "print(f'Accuracy on train data{100*correct/total:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AywyLWa7KGSB",
        "outputId": "4ad3b9af-04e5-4c1e-8ea2-dacf2d3907b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on train data94.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kus8pu9TKGVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C7WvgO4RKGX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EvPZ5NN0KGap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FjP7ObPSKGdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fcs73gUYKGgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xlsez1oRKGjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8oROt3IkKGmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zLcQzP1DKGol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GdTC34D-KGrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m7avDG7EKGuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wYMpSfGvJSvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6XgfsYvglm0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yos1VvM7lm3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kiqP62Bhlm63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sb9HWKXHlm-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs.data = torch.tensor([\n",
        "#     [2.5, 1.2, 0.8, 5.0],  # Image 1\n",
        "#     [3.0, 2.5, 1.2, 1.1]   # Image 2\n",
        "# ])"
      ],
      "metadata": {
        "id": "S0OGDwzvlnBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _, predicted = torch.max(outputs.data, 1)\n"
      ],
      "metadata": {
        "id": "9DA88jEIlnD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicted = torch.tensor([9, 0])\n",
        "# This means that the model predicted class 9 for the first example and class 0 for the second example"
      ],
      "metadata": {
        "id": "T8hGfq-7YuSA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}